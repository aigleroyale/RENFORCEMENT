{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "rl (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b1c86df"
      },
      "source": [
        "Maintenant expliquons l'implémentation complète de ce modèle Q-Learning, la solution de notre problème d'optimisation des flux de stockage.\n",
        "Tout d'abord, nous commençons par importer les modules qui seront utilisées dans cette implémentation.\n",
        "Seule NumPy offre des outils pratiques avec des tableaux et des opérations mathématiques"
      ],
      "id": "0b1c86df"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "777ee586"
      },
      "source": [
        "# Importation des bibliothèques\n",
        "import numpy as np"
      ],
      "id": "777ee586",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ab08faf"
      },
      "source": [
        "Ensuite, nous défnissons les paramètres de notre modèle. Il s'agit du facteur de réduction γ gamma et du taux\n",
        "d'apprentissage α alpha qui sont les seuls paramètres de l'algorithme du Q-Learning :"
      ],
      "id": "7ab08faf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c66359a"
      },
      "source": [
        "# Réglage des paramètres gamma et alpha pour le Q-Learning\n",
        "gamma = 0.75\n",
        "alpha = 0.9"
      ],
      "id": "0c66359a",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22a0d44d"
      },
      "source": [
        "Nous commençons par défnir les états avec un dictionnaire indiquant les noms des emplacements (lettres de A à L) dans les états (index de 0 à 11) :"
      ],
      "id": "22a0d44d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f161ce64"
      },
      "source": [
        "# DÉFINIR L'ENVIRONNEMENT\n",
        "# Définir les états\n",
        "location_to_state = {   'A': 0,\n",
        "                        'B': 1,\n",
        "                        'C': 2,\n",
        "                        'D': 3,\n",
        "                        'E': 4,\n",
        "                        'F': 5,\n",
        "                        'G': 6,\n",
        "                        'H': 7,\n",
        "                        'I': 8,\n",
        "                        'J': 9,\n",
        "                        'K': 10,\n",
        "                        'L':  11 }"
      ],
      "id": "f161ce64",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25f64299"
      },
      "source": [
        "Ensuite nous définissons les actions avec une simple liste d'index de 0 à 11. Rappelons-nous que chaque index\n",
        "d'action correspond à l'état suivant (emplacement suivant) où cette action conduit à :"
      ],
      "id": "25f64299"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ecb3f27"
      },
      "source": [
        "# Définir les actions\n",
        "actions = [0,1,2,3,4,5,6,7,8,9,10,11]"
      ],
      "id": "1ecb3f27",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dc83892"
      },
      "source": [
        "Finalement, nous définissons les récompenses en créant une matrice dont les lignes correspondent aux états\n",
        "actuels $s_t$, les colonnes aux actions $a_t$ menant à l'état suivant $s_{t+1}$ et les cellules contiennent les récompenses\n",
        "$R(s_t, a_t)$. Si une cellule $(s_t, a_t)$ a 1, cela signifie que nous pouvons exécuter l'action $a_t$ depuis l'état actuel $s_t$ pour atteindre l'état suivant $s_{t+1}$. Si une cellule $(s_t, a_t)$ a 0, cela signifie que nous ne pouvons pas exécuter\n",
        "l'action $a_t$ depuis l'état actuel $s_t$ pour atteindre l'état suivant $s_{t+1}$. Nous allons mettre manuellement\n",
        "une récompense élevée (1000) dans la cellule correspondant à l'emplacement `G`, car c'est l'emplacement\n",
        "prioritaire où le robot autonome doit récupérer les produits. Puisque l'emplacement `G` a encodé l'état avec\n",
        "l'index 6, nous avons mis une récompense de 1000 dans la cellule de la ligne 6 et de la colonne 6. Ensuite,\n",
        "nous développerons notre solution en implémentant un système automatique pour aller à l'emplacement\n",
        "prioritaire sans avoir à mettre à jour manuellement la matrice de récompenses et en la laissant initialisée\n",
        "avec 0 et 1 comme elle devrait l'être. Voici notre matrice de récompenses incluant la mise à jour manuelle :"
      ],
      "id": "5dc83892"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc37bfb3"
      },
      "source": [
        "# Définir les récompenses\n",
        "R = np.array([  [0,1,0,0,0,0,0,0,0,0,0,0],\n",
        "                [1,0,1,0,0,1,0,0,0,0,0,0],\n",
        "                [0,1,0,0,0,0,1,0,0,0,0,0],\n",
        "                [0,0,0,0,0,0,0,1,0,0,0,0],\n",
        "                [0,0,0,0,0,0,0,0,1,0,0,0],\n",
        "                [0,1,0,0,0,0,0,0,0,1,0,0],\n",
        "                [0,0,1,0,0,0,1000,1,0,0,0,0],\n",
        "                [0,0,0,1,0,0,1,0,0,0,0,1],\n",
        "                [0,0,0,0,1,0,0,0,0,1,0,0],\n",
        "                [0,0,0,0,0,1,0,0,1,0,1,0],\n",
        "                [0,0,0,0,0,0,0,0,0,1,0,1],\n",
        "                [0,0,0,0,0,0,0,1,0,0,1,0]])"
      ],
      "id": "fc37bfb3",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0fe5f7f"
      },
      "source": [
        "Dans cette partie nous devons Développer la solution d'IA avec le Q-Learning. Pour ce faire, nous allons utiliser l'algorithme de Q-Learning exactement tel qu'il a été fourni dans notre support. Nous commençons donc par initialiser toutes les Q-Values en créant notre matrice de Q-Values contenant des zéros (dans laquelle les lignes correspondent aux états actuels $s_t$, les colonnes aux actions $a_t$ menant aux états suivants $s_{t+1}$ et les cellules contiennent les Q-Values $Q(s_t, a_t)$ :"
      ],
      "id": "a0fe5f7f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3ba04dd"
      },
      "source": [
        "# Initialisation des valeurs Q\n",
        "Q = np.array(np.zeros([12,12]))"
      ],
      "id": "d3ba04dd",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "744236a5"
      },
      "source": [
        "Ensuite, nous implémentons le processus de Q-Learning avec une boucle pour plus de 1000 itérations, répétant\n",
        "1000 fois les étapes de notre processus de Q-Learning."
      ],
      "id": "744236a5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43964917"
      },
      "source": [
        "# Mise en œuvre du processus Q-Learning\n",
        "for i in range(1000):\n",
        "    current_state = np.random.randint(0,12)\n",
        "    playable_actions = []\n",
        "    for j in range(12):\n",
        "        if R[current_state, j] > 0:\n",
        "            playable_actions.append(j)\n",
        "    next_state = np.random.choice(playable_actions)\n",
        "    TD = R[current_state, next_state] + gamma*Q[next_state, np.argmax(Q[next_state,])] - Q[current_state, next_state]\n",
        "    Q[current_state, next_state] = Q[current_state, next_state] + alpha*TD"
      ],
      "id": "43964917",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0af68c49"
      },
      "source": [
        "Facultatif : à ce stade du code, notre matrice de Q-Values est prête. Nous pouvons y jeter un coup d'oeil en\n",
        "exécutant le code que nous avons implémenté jusqu'à présent et en entrant le print suivant dans la console :"
      ],
      "id": "0af68c49"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "785ba1da",
        "outputId": "2fec9d10-df50-4422-ae8a-f38c7173d59a"
      },
      "source": [
        "print(\"Q-Values:\")\n",
        "print(Q.astype(int))"
      ],
      "id": "785ba1da",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Values:\n",
            "[[   0 1661    0    0    0    0    0    0    0    0    0    0]\n",
            " [1243    0 2213    0    0 1246    0    0    0    0    0    0]\n",
            " [   0 1661    0    0    0    0 2950    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0 2213    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0  697    0    0    0]\n",
            " [   0 1661    0    0    0    0    0    0    0  905    0    0]\n",
            " [   0    0 2212    0    0    0 3947 2212    0    0    0    0]\n",
            " [   0    0    0 1661    0    0 2950    0    0    0    0 1653]\n",
            " [   0    0    0    0  447    0    0    0    0  928    0    0]\n",
            " [   0    0    0    0    0 1238    0    0  679    0 1189    0]\n",
            " [   0    0    0    0    0    0    0    0    0  930    0 1660]\n",
            " [   0    0    0    0    0    0    0 2213    0    0 1246    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d190e162"
      },
      "source": [
        "Ensuite vient la mise en production, dans laquelle nous calculerons le chemin optimal de n'importe quel point de\n",
        "départ à n'importe quel point final de priorité absolue. L'idée est de mettre en place une fonction \"route\"\n",
        "qui prendra comme entrées le point de départ où se trouve notre robot autonome à un temps donné et\n",
        "le point d'arrivée où il doit aller en priorité absolue, et qui donnera comme sortie le chemin le plus court\n",
        "dans une liste. Cependant, puisque nous voulons entrer les emplacements avec leurs noms (en lettres), par\n",
        "opposition à leurs états (en index), nous aurons besoin d'un dictionnaire qui associe les états (en index) aux\n",
        "emplacements (en lettres). Et c'est la première chose que nous allons faire ici en utilisant une technique\n",
        "pour inverser notre dictionnaire précédent \"location_to_state\", puisque nous voulons simplement obtenir\n",
        "le mapping inverse exact à partir de ce dictionnaire :"
      ],
      "id": "d190e162"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53be7264"
      },
      "source": [
        "# Faire une cartographie des états aux lieux.\n",
        "state_to_location = {state: location for location, state in location_to_state.items()}"
      ],
      "id": "53be7264",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaa981b3"
      },
      "source": [
        "Ainsi intervient la section la plus importante du code. Nous sommes sur le point d'implémenter la fonction\n",
        "finale **\"route()\"** qui aura comme entrées les emplacements de départ et d'arrivée, et qui donne l'itinéraire\n",
        "optimal entre ces deux emplacements. Pour expliquer exactement ce que fera cette fonction route, nous\n",
        "allons énumérer les différentes étapes du processus, en allant de l'emplacement E à l'emplacement G :\n",
        "\n",
        "1. Nous commençons à notre emplacement de départ E.\n",
        "\n",
        "\n",
        "2. Nous obtenons l'état de l'emplacement E (d'après notre mapping location_to_state) est $s_0$ = 4.\n",
        "\n",
        "\n",
        "3. Sur la ligne d'index $s_0$ = 4 dans notre matrice de Q-Values, nous trouvons la colonne qui a la Q-Value maximale.\n",
        "\n",
        "\n",
        "4. Cette colonne a l'index 8, donc nous exécutons l'action de l'index 8 qui mène à l'état suivant $s_{t+1}$ = 8.\n",
        "\n",
        "\n",
        "5. Nous obtenons l'emplacement de l'état 8, qui selon notre mapping state_to_location est I. AInsi, notre prochain emplacement est I qui est annexé à notre liste contenant le chemin optimal.\n",
        "\n",
        "\n",
        "6. Nous répétons les 5 étapes précédentes à partir de notre nouveau point de départ I jusqu'à ce que nous atteignions notre destination finale, le point G.\n",
        "\n",
        "\n",
        "Par conséquent, comme nous ne savons pas combien d'emplacements nous devrons traverser entre le point\n",
        "de départ et d'arrivée, nous devons faire une boucle while qui répétera le processus en 5 étapes décrites\n",
        "ci-dessus, et qui s'arrêtera dès que nous atteindrons l'emplacement final de priorité supérieure :\n",
        "\n",
        "`NB :` ce processus est très long"
      ],
      "id": "aaa981b3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fa7a2b2"
      },
      "source": [
        "# Faire la fonction finale qui retournera la route optimale\n",
        "def route(starting_location, ending_location):\n",
        "    route = [starting_location]\n",
        "    next_location = starting_location\n",
        "    while (next_location != ending_location):\n",
        "        starting_state = location_to_state[starting_location]\n",
        "        next_state = np.argmax(Q[starting_state,])\n",
        "        next_location = state_to_location[next_state]\n",
        "        route.append(next_location)\n",
        "        starting_location = next_location\n",
        "    return route"
      ],
      "id": "1fa7a2b2",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac793aa4"
      },
      "source": [
        "L'outil est prêt ! Quand on le teste pour passer de E à G, on obtient les deux itinéraires optimaux\n",
        "possibles après la définition de l'itinéraire final en exécutant le code complet plusieurs fois :"
      ],
      "id": "ac793aa4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54b05b0f",
        "outputId": "f957d043-01ac-4f86-81d2-9a3b4c7b5907"
      },
      "source": [
        "print('Route:')\n",
        "route('E', 'G')"
      ],
      "id": "54b05b0f",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['E', 'I', 'J', 'F', 'B', 'C', 'G']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a075d080"
      },
      "source": [
        "Bien, nous avons une première version du modèle pratique. Mais nous pouvons l'améliorer de deux façons.\n",
        "\n",
        "1. Automatiser l'attribution de récompense à l'emplacement prioritaire afin que nous n'ayons pas à le faire manuellement. \n",
        "\n",
        "2. Deuxièmement, en ajoutant une fonction qui nous donne la possibilité de passer par un emplacement intermédiaire avant d'aller à l'emplacement prioritaire. \n",
        "\n",
        "Cet emplacement intermédiaire devrait bien sûr figurer parmi les 3 premiers emplacements prioritaires. Dans notre classement des emplacements prioritaires, le deuxième est K. Par conséquent, afin d'optimiser encore plus les flux de stockage, notre robot autonome doit passer par l'emplacement K pour récupérer les produits en route vers l'emplacement\n",
        "prioritaire G. Pour ce faire, il est possible de passer par tout emplacement intermédiaire dans le cadre de notre\n",
        "fonction **\"route()\"**. Et c'est exactement ce que nous allons mettre en oeuvre comme deuxième amélioration.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "a075d080"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26fc577c"
      },
      "source": [
        "Mais d'abord, mettons en oeuvre la première amélioration qui automatise l'attribution des récompenses.\n",
        "Il faut d'abord faire une copie de notre matrice de récompenses (appelée R_new) dans laquelle la fonction\n",
        "route() mettra automatiquement à jour la récompense dans la cellule de l'emplacement final. En effet,\n",
        "l'emplacement final est l'une des entrées de la fonction route(), donc en utilisant notre dictionnaire location_\n",
        "to_state, nous pouvons très facilement trouver cette cellule et mettre à jour sa récompense à 1000.\n",
        "\n",
        "\n",
        "Deuxièmement, nous devons inclure l'algorithme complet du Q-Learning (y compris l'étape d'initialisation)\n",
        "dans la fonction route, juste après la mise à jour de la récompense dans notre copie. Dans notre implémentation\n",
        "précédente, le processus de Q-Learning se déroule dans la version originale de la matrice de récompenses\n",
        "qui est maintenant censée rester telle qu'elle, c.-à-d. initialisée à 1 et 0 seulement. Par conséquent, nous\n",
        "devons inclure le processus de Q-Learning dans la fonction route et le reproduire dans notre copie R_new\n",
        "de la matrice de récompenses au lieu de l'originale R. Ainsi, notre implémentation devient la suivante :"
      ],
      "id": "26fc577c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7afc317f"
      },
      "source": [
        "# Importer les bibliothèques\n",
        "import numpy as np\n",
        "\n",
        "# Réglage des paramètres gamma et alpha pour le Q-Learning\n",
        "gamma = 0.75\n",
        "alpha = 0.9\n",
        "\n",
        "# DÉFINIR L'ENVIRONNEMENT\n",
        "# Définir les états\n",
        "location_to_state = {   'A': 0,\n",
        "                        'B': 1,\n",
        "                        'C': 2,\n",
        "                        'D': 3,\n",
        "                        'E': 4,\n",
        "                        'F': 5,\n",
        "                        'G': 6,\n",
        "                        'H': 7,\n",
        "                        'I': 8,\n",
        "                        'J': 9,\n",
        "                        'K': 10,\n",
        "                        'L':  11 }\n",
        "\n",
        "# Définir les actions\n",
        "actions = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
        "\n",
        "# Définir les récompenses\n",
        "R = np.array([  [0,1,0,0,0,0,0,0,0,0,0,0],\n",
        "                [1,0,1,0,0,1,0,0,0,0,0,0],\n",
        "                [0,1,0,0,0,0,1,0,0,0,0,0],\n",
        "                [0,0,0,0,0,0,0,1,0,0,0,0],\n",
        "                [0,0,0,0,0,0,0,0,1,0,0,0],\n",
        "                [0,1,0,0,0,0,0,0,0,1,0,0],\n",
        "                [0,0,1,0,0,0,1,1,0,0,0,0],\n",
        "                [0,0,0,1,0,0,1,0,0,0,0,1],\n",
        "                [0,0,0,0,1,0,0,0,0,1,0,0],\n",
        "                [0,0,0,0,0,1,0,0,1,0,1,0],\n",
        "                [0,0,0,0,0,0,0,0,0,1,0,1],\n",
        "                [0,0,0,0,0,0,0,1,0,0,1,0]])\n",
        "\n",
        "# Faire une cartographie des états aux lieux.\n",
        "state_to_location = {state: location for location, state in location_to_state.items()}\n",
        "\n",
        "# Faire la fonction finale qui retournera la route optimale\n",
        "def route(starting_location, ending_location):\n",
        "    R_new = np.copy(R)\n",
        "    ending_state = location_to_state[ending_location]\n",
        "    Q = np.array(np.zeros([12,12]))\n",
        "    for i in range(2):\n",
        "      current_state = np.random.randint(0,12)\n",
        "    playable_actions = []\n",
        "    for j in range(12):\n",
        "        if R_new[current_state, j] > 0:\n",
        "            playable_actions.append(j)\n",
        "    next_state = np.random.choice(playable_actions)\n",
        "    TD = R_new[current_state, next_state] + gamma*Q[next_state, np.argmax(Q[next_state,])] - Q[current_state, next_state]\n",
        "    Q[current_state, next_state] = Q[current_state, next_state] + alpha*TD\n",
        "    \n",
        "    route = [starting_location]\n",
        "    next_location = starting_location\n",
        "    while (next_location != ending_location):\n",
        "        starting_state = location_to_state[starting_location]\n",
        "        next_state = np.argmax(Q[starting_state,])\n",
        "        next_location = state_to_location[next_state]\n",
        "        route.append(next_location)\n",
        "        starting_location = next_location\n",
        "    return route\n"
      ],
      "id": "7afc317f",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "652c916c",
        "outputId": "b6efb64a-744c-4ff4-a9bd-3800db87be5d"
      },
      "source": [
        "print('Route:')\n",
        "route('E', 'G')"
      ],
      "id": "652c916c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27dc8262"
      },
      "source": [
        "Passons maintenant à la deuxième amélioration. Il y a trois façons d'ajouter l'option de passer par l'emplacement\n",
        "intermédiaire K, le deuxième emplacement prioritaire :\n",
        "\n",
        "\n",
        "1. Nous donnons une récompense élevée à l'action menant de l'emplacement J à K. Cette récompense élevée doit être supérieure à 1, et inférieure à 1000. En effet, elle doit être supérieure à 1 pour que le processus de Q-Learning favorise l'action menant de J à K, par opposition à l'action menant de J à F qui a la récompense 1. Et elle doit être inférieure à 1000, de sorte que nous devons conserver la récompense la plus élevée dans l'emplacement prioritaire. Ainsi, dans notre matrice, nous pouvons donner une récompense élevée de 500 à la cellule dans la ligne de l'index 9 et la colonne de l'index 10, puisque cette cellule correspond effectivement à l'action menant de l'emplacement J. Voici comment se présente la matrice de récompenses dans ce cas :"
      ],
      "id": "27dc8262"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8e2cfb4"
      },
      "source": [
        "R = np.array([  [0,1,0,0,0,0,0,0,0,0,0,0],\n",
        "                [1,0,1,0,0,1,0,0,0,0,0,0],\n",
        "                [0,1,0,0,0,0,1,0,0,0,0,0],\n",
        "                [0,0,0,0,0,0,0,1,0,0,0,0],\n",
        "                [0,0,0,0,0,0,0,0,1,0,0,0],\n",
        "                [0,1,0,0,0,0,0,0,0,1,0,0],\n",
        "                [0,0,1,0,0,0,1,1,0,0,0,0],\n",
        "                [0,0,0,1,0,0,1,0,0,0,0,1],\n",
        "                [0,0,0,0,1,0,0,0,0,1,0,0],\n",
        "                [0,0,0,0,0,1,0,0,1,0,500,0],\n",
        "                [0,0,0,0,0,0,0,0,0,1,0,1],\n",
        "                [0,0,0,0,0,0,0,1,0,0,1,0]])"
      ],
      "id": "c8e2cfb4",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "324045e5"
      },
      "source": [
        "2. Nous donnons une faible récompense à l'action menant de l'emplacement J à F. Elle doit juste être inférieure à 0. En effet, en sanctionnant cette action avec une faible récompense, le processus de Q-Learning ne favorisera jamais cette action menant de J à F. Dans notre matrice de récompenses nous pouvons donner une mauvaise récompense de -500 à la cellule de la ligne de l'index 9 et la colonne de l'index 5, car cette cellule correspond à l'action allant de l'emplacement J (index 9) vers F (index 5). De cette façon, notre robot autonome ne passera jamais par l'emplacement F pour se rendre à G. Voici comment se présente la matrice de récompenses dans ce cas :\n"
      ],
      "id": "324045e5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9e85cf0"
      },
      "source": [
        "R = np.array([  [0,1,0,0,0,0,0,0,0,0,0,0],\n",
        "                [1,0,1,0,0,1,0,0,0,0,0,0],\n",
        "                [0,1,0,0,0,0,1,0,0,0,0,0],\n",
        "                [0,0,0,0,0,0,0,1,0,0,0,0],\n",
        "                [0,0,0,0,0,0,0,0,1,0,0,0],\n",
        "                [0,1,0,0,0,0,0,0,0,1,0,0],\n",
        "                [0,0,1,0,0,0,1,1,0,0,0,0],\n",
        "                [0,0,0,1,0,0,1,0,0,0,0,1],\n",
        "                [0,0,0,0,1,0,0,0,0,1,0,0],\n",
        "                [0,0,0,0,0,-500,0,0,1,0,1,0],\n",
        "                [0,0,0,0,0,0,0,0,0,1,0,1],\n",
        "                [0,0,0,0,0,0,0,1,0,0,1,0]])"
      ],
      "id": "e9e85cf0",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e457708"
      },
      "source": [
        "Les deux premières idées sont faciles à implémenter manuellement, mais très délicates automatiquement. Il\n",
        "est facile de trouver automatiquement l'index de l'emplacement intermédiaire, mais très difficile d'obtenir\n",
        "l'index qui mène à cet emplacement, car cela dépend de l'emplacement initial et final. \n",
        "\n",
        "En conséquence, nous allons mettre en oeuvre la 3e idée qui peut être codée en seulement deux lignes supplémentaires de code :"
      ],
      "id": "0e457708"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "5fb59279",
        "outputId": "b6ca265b-a700-4e7e-f88d-fefed295984d"
      },
      "source": [
        "# Faire la fonction finale qui retourne la route optimale\n",
        "def best_route(starting_location, intermediary_location, ending_location):\n",
        "    return route(starting_location, intermediary_location)\n",
        "            + route(intermediary_location, ending_location)[1:]"
      ],
      "id": "5fb59279",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-6bebbb81483b>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    + route(intermediary_location, ending_location)[1:]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3ajiNHs64Oa"
      },
      "source": [
        ""
      ],
      "id": "t3ajiNHs64Oa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ce2547c"
      },
      "source": [
        "Le code final incluant cette amélioration majeure pour l'optimisation des flux de notre entrepôts devient :"
      ],
      "id": "0ce2547c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42298240"
      },
      "source": [
        "# Importer les bibliothèques\n",
        "import numpy as np\n",
        "\n",
        "# Réglage des paramètres gamma et alpha pour le Q-Learning\n",
        "gamma = 0.75\n",
        "alpha = 0.9\n",
        "\n",
        "# PARTIE 1 - DÉFINIR L'ENVIRONNEMENT\n",
        "# Définir les états\n",
        "location_to_state = {   'A': 0,\n",
        "                        'B': 1,\n",
        "                        'C': 2,\n",
        "                        'D': 3,\n",
        "                        'E': 4,\n",
        "                        'F': 5,\n",
        "                        'G': 6,\n",
        "                        'H': 7,\n",
        "                        'I': 8,\n",
        "                        'J': 9,\n",
        "                        'K': 10,\n",
        "                        'L':  11 }\n",
        "\n",
        "# Définir les actions\n",
        "actions = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
        "\n",
        "# Définir les récompenses\n",
        "R = np.array([  [0,1,0,0,0,0,0,0,0,0,0,0],\n",
        "                [1,0,1,0,0,1,0,0,0,0,0,0],\n",
        "                [0,1,0,0,0,0,1,0,0,0,0,0],\n",
        "                [0,0,0,0,0,0,0,1,0,0,0,0],\n",
        "                [0,0,0,0,0,0,0,0,1,0,0,0],\n",
        "                [0,1,0,0,0,0,0,0,0,1,0,0],\n",
        "                [0,0,1,0,0,0,1,1,0,0,0,0],\n",
        "                [0,0,0,1,0,0,1,0,0,0,0,1],\n",
        "                [0,0,0,0,1,0,0,0,0,1,0,0],\n",
        "                [0,0,0,0,0,1,0,0,1,0,1,0],\n",
        "                [0,0,0,0,0,0,0,0,0,1,0,1],\n",
        "                [0,0,0,0,0,0,0,1,0,0,1,0]])\n",
        "\n",
        "# Faire une cartographie des états aux lieux.\n",
        "state_to_location = {state: location for location, state in location_to_state.items()}\n",
        "\n",
        "# Faire la fonction finale qui retournera la route optimale\n",
        "def route(starting_location, ending_location):\n",
        "    R_new = np.copy(R)\n",
        "    ending_state = location_to_state[ending_location]\n",
        "    Q = np.array(np.zeros([12,12]))\n",
        "    for i in range(2):\n",
        "    current_state = np.random.randint(0,12)\n",
        "    playable_actions = []\n",
        "    for j in range(12):\n",
        "        if R_new[current_state, j] > 0:\n",
        "            playable_actions.append(j)\n",
        "    next_state = np.random.choice(playable_actions)\n",
        "    TD = R_new[current_state, next_state] \n",
        "         + gamma*Q[next_state, np.argmax(Q[next_state,])] \n",
        "         - Q[current_state, next_state]\n",
        "    Q[current_state, next_state] = Q[current_state, next_state] + alpha*TD\n",
        "\n",
        "    route = [starting_location]\n",
        "    next_location = starting_location\n",
        "    while (next_location != ending_location):\n",
        "        starting_state = location_to_state[starting_location]\n",
        "        next_state = np.argmax(Q[starting_state,])\n",
        "        next_location = state_to_location[next_state]\n",
        "        route.append(next_location)\n",
        "        starting_location = next_location\n",
        "    return route\n",
        "\n",
        "# Faire la fonction finale qui retourne la route optimale\n",
        "def best_route(starting_location, intermediary_location, ending_location):\n",
        "    return route(starting_location, intermediary_location)\n",
        "            + route(intermediary_location, ending_location)[1:]"
      ],
      "id": "42298240",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f7307fa"
      },
      "source": [
        "# Affichage de la route finale\n",
        "print('Route:')\n",
        "best_route('E', 'K', 'G')"
      ],
      "id": "9f7307fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f049cbd8"
      },
      "source": [
        "Fin de notre programme !!!"
      ],
      "id": "f049cbd8"
    }
  ]
}